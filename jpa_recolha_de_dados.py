# -*- coding: utf-8 -*-
"""JPA_Recolha de dados.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGQOJCWLtPOH0t_FpXTylyd3HVOW5y9w

Atualmente trabalho numa empresa de importação e exportação de peças auto, onde o mercado se encontra a mudar constantemente, havendo uma considerável percentagem de lojas de peças auto e oficinas que vão fechando enquanto outras vão iniciando atividade.

Considerando importante o acompanhamento das empresas com os seus clientes, mas mais importante ainda a aquisição de novos clientes, de onde surgem questões como:


*   Quem poderão ser os novos clientes?
*   Onde se encontram possíveis novos clientes e como poderei entrar em contacto?

Dado este paradigma este trabalho vai-se centrar em recolher informações de possíveis clientes que tenham o cae Nº45320 destinado ao comércio a retalho de peças e acessórios para veículos automóveis.

O objectivo passa numa primeira fase por adquirir o nome, o número de contribuinte e a localização da loja com informações da freguesia, concelho e distrito utilizando a técnica de web scrape na página https://www.racius.com/pecas-e-acessorios/em-atividade/, a informação irá ser guardada numa tabela na base de dados.

Após esta operação irei utilizar a API disponível em https://www.nif.pt/api/ onde irei recolher a restante informação disponível acerca das empresas. Irei utilizar os endpoints "q" que realiza uma pesquisa de um determinado NIF, "credits" de modo a verificar os créditos disponíveis gratuitamente e o endpoint buy de modo a simular a aquisição de créditos onde retorna informação das referências de multibanco para efetuar pagamento.
"""

import sqlite3
import pandas as pd
db = 'python.sqlite.db'

from google.colab import drive
drive.mount('/content/drive')

#Criar base de dados e respectiva estrutura de dados
conn = sqlite3.connect('new_clients.db')
cur = conn.cursor()

query = """ CREATE TABLE IF NOT EXISTS clients (
                                    nif int PRIMARY KEY,
                                    nome varchar,
                                    freguesia varchar,
                                    concelho varchar,
                                    distrito varchar,
                                    morada varchar,
                                    cod_postal varchar,
                                    email varchar,
                                    telefone varchar,
                                    website varchar,
                                    fax varchar,
                                    status varchar
                                ); """
cur.execute(query)

import requests # pedidos http 
from bs4 import BeautifulSoup # web scraping
import json
import time #vai servir para não sobrecarregar a APi com pedidos aproveitando os pedidos gratuitos.

def get_data(url, parse):
    """
    returns a parsed html 
    """
    raw_html = requests.get(url, parse).content
    return BeautifulSoup(raw_html)

query_insert = """
    INSERT INTO clients
    (nif, nome, freguesia, concelho, distrito, morada, cod_postal, email, telefone, website, fax, status)
    VALUES ({}, '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}')
"""

#Iniciar a técnica de web scrape para recolha de dados
html = get_data('https://www.racius.com/pecas-e-acessorios/em-atividade/1/', 'html.parse')

#Primeiro vou recolher o número total de páginas para conseguir guardar todos os registros disponíveis
pages = html.findAll("div", {"class": "pull-right text-right pagination-totals "}) 
pages = pages[0].get_text().lstrip().rstrip().split()[-1]
pages

page=1
while page < int(pages):
  html = get_data('https://www.racius.com/pecas-e-acessorios/em-atividade/'+str(page)+'/', 'html.parse')
  page = page+1

  #Para cada página vou procurar todas as empresas
  items = html.findAll("div", {"id": "allresults"})

  for index in range(len(items[0].find_all(attrs = {"class":"title"}))):
    #Nome da empresas
    nome = items[0].find_all(attrs = {"class":"title"})[index].get_text()

    #verifica se tem informação de freguesia, concelho, distrito se não tiver insere a vazio e posteriormente é inserido através da API
    if len(items[0].find_all(attrs = {"class":"local"})[index].get_text().split(',')) == 3:
      freguesia, concelho, distrito = items[0].find_all(attrs={"class":"local"})[index].get_text().replace("'",'').split(',')
    else:
      freguesia, concelho, distrito = '','',''
    
    #Determina o Nif da empresa
    nif = items[0].find_all(attrs={"class":"col-md-12"})[index]
    nif = nif.find_all("span")
    nif = nif[1].get_text().split()[1]
   
    #Insere os valores obtidos os restantes campos são inseridos a vazio para posteriormente serem preenchidos
    cur.execute(query_insert.format(int(nif), nome, freguesia, concelho, distrito, '', '', '', '', '', '', '' ))
    
conn.commit()

#Vou selecionar os clientes que tem dados por preencher através da API tais como morada, telefone (considero ser os principais para poder comunicar com o futuro cliente)
#Vou limitar a 10 porque é o limite por hora de requests independentemente de o resultado da mesma for de sucesso ou erro
query_sel = """
    SELECT  * 
    FROM clients where morada='' and telefone='' limit 10
"""
cur.execute(query_sel)
newcl=cur.fetchall()

#utilizar a Api de nif.pt dado ser um serviço gratuito e limitado vou colocar a key vísivel para testares
key = '11982fa91b7e5209f6d9317470682614'
url = "http://www.nif.pt/?json=1"

#Vou validar a existência de créditos caso não exista créditos fornece os dados para pagamento através de MB.
param_credits = {'credits':'1',
                 'key': key}

response_credits = requests.request("GET", url,  params=param_credits) 
credits_ = response_credits.json()

if credits_['credits']['hour']==0:
  param_compra = {'buy':'1000',
                  'key': key}
  response_compra = requests.request("GET", url_,  params=param_compra) 
  compra_ = response_compra.json()
  print("Dados para pagamento:\n Entidade: "+compra_['mb']['entity']+"\n Referência:"+compra_['mb']['reference']+"\n Valor: "+compra_['mb']['amount'])
else:
  print("Ainda possui este mês: "+str(credits_['credits']['month'])+" No dia de hoje: "+str(credits_['credits']['month'])+"\n Na Hora corrente: "+str(credits_['credits']['hour'])+"\n No minuto corrente: "+str(credits_['credits']['minute'])+"\n Créditos adquiridos disponíveis: "+str(credits_['credits']['paid']))

#Este processo pode ter a duração máxima de 10min caso tenhamos o número máximo de créditos por hora disponíveis
for cl in newcl:
  querystring = {'q':cl[0],
                 'key': key}
  response_w = requests.request("GET", url,  params=querystring)
  r = response_w.json()

  if r['result'] == 'success':
    #vou recolher a informação do json
    freguesia=r['records'][str(cl[0])]['geo']['parish']
    concelho=r['records'][str(cl[0])]['geo']['county']
    distrito=r['records'][str(cl[0])]['geo']['region']
    morada=r['records'][str(cl[0])]['place']['address']
    cod_postal=r['records'][str(cl[0])]['place']['pc4']+'-'+r['records'][str(cl[0])]['place']['pc3']
    email=r['records'][str(cl[0])]['contacts']['email']
    telefone=r['records'][str(cl[0])]['contacts']['phone']
    website=r['records'][str(cl[0])]['contacts']['website']
    fax=r['records'][str(cl[0])]['contacts']['fax']
    status=r['records'][str(cl[0])]['status']

    #Valido se existe necessidade alterar os campos da freguesia, concelho e distrito.
    if cl[2] == '':
      query_upd = """
      update clients set morada = '{}', cod_postal = '{}', email = '{}', telefone = '{}', website = '{}', fax = '{}', status = '{}', freguesia = '{}', concelho = '{}', distrito = '{}'
      where nif = {}
      """
      cur.execute(query_upd.format(morada, cod_postal, email, telefone, website, fax, status, freguesia, concelho, distrito, int(cl[0])))

    else:
      query_upd = """
      update clients set morada = '{}', cod_postal = '{}', email = '{}', telefone = '{}', website = '{}', fax = '{}', status = '{}'
      where nif = {}
      """
      cur.execute(query_upd.format(morada, cod_postal, email, telefone, website, fax, status, int(cl[0]) ))
      #o Webservice só permite 1 por minuto, por isso faço uma pausa no ciclo de 61 segundos para ele realizar 10 updates (Dado ser o limite por hora.)
      time.sleep(61) 
    conn.commit()
  else:
    #Caso dê algum erro ele imprime a mensagem do erro da API visualizando o número de créditos disponíveis.
    #Como funcionam os créditos: Temos 1000 mensais, 100 diários, 10 por hora e 1 por minuto, pode acontecer de termos disponível o minuto mas não o da hora aí a API não vai retornar resultados.
   
    param_credits = {'credits':'1',
               'key': key}
    response_credits = requests.request("GET", url,  params=param_credits) 
    credits_ = response_credits.json()
    print(r['message'], credits_)
    time.sleep(61)

#Verificação do preenchimento de informação na base de dados
query_sel_check = """
    SELECT  * 
    FROM clients limit 10
"""
cur.execute(query_sel_check)
cur.fetchall()

